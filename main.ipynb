{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b88c8f0f-2763-434f-a4a6-0da569247caa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Introduction to Random Forest algorithm*\n",
    "\n",
    "Random forest is a supervised learning algorithm. It has two variations – one is used for classification problems and other is used for regression problems. It is one of the most flexible and easy to use algorithm. It creates decision trees on the given data samples, gets prediction from each tree and selects the best solution by means of voting. It is also a pretty good indicator of feature importance.\n",
    "\n",
    "\n",
    "Random forest algorithm combines multiple decision-trees, resulting in a forest of trees, hence the name `Random Forest`. In the random forest classifier, the higher the number of trees in the forest results in higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f1699c3-fd2e-4c87-a68d-68ef86a9bb6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Random Forest algorithm intuition\n",
    "\n",
    "Random forest algorithm intuition can be divided into two stages. \n",
    "\n",
    "In the first stage, we randomly select “k” features out of total `m` features and build the random forest. In the first stage, we proceed as follows:-\n",
    "\n",
    "1. Randomly select `k` features from a total of `m` features where `k < m`.\n",
    "2. Among the `k` features, calculate the node `d` using the best split point.\n",
    "3. Split the node into daughter nodes using the best split.\n",
    "4. Repeat 1 to 3 steps until `l` number of nodes has been reached.\n",
    "5. Build forest by repeating steps 1 to 4 for `n` number of times to create `n` number of trees.\n",
    "\n",
    "In the second stage, we make predictions using the trained random forest algorithm. \n",
    "\n",
    "1. We take the test features and use the rules of each randomly created decision tree to predict the outcome and stores the predicted outcome.\n",
    "2. Then, we calculate the votes for each predicted target.\n",
    "3. Finally, we consider the high voted predicted target as the final prediction from the random forest algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae8585e8-571b-4d7a-b022-52d7abe11b4d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Random Forest algorithm intuition\n",
    "\n",
    "![Random Forest](https://i.ytimg.com/vi/goPiwckWE9M/maxresdefault.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d51be4-6f05-423f-b210-2fa79eafd574",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Advantages and disadvantages of Random Forest algorithm\n",
    "\n",
    "The advantages of Random forest algorithm are as follows:-\n",
    "\n",
    "\n",
    "1. Random forest algorithm can be used to solve both classification and regression problems.\n",
    "2. It is considered as very accurate and robust model because it uses large number of decision-trees to make predictions.\n",
    "3. Random forests takes the average of all the predictions made by the decision-trees, which cancels out the biases. So, it does not suffer from the overfitting problem. \n",
    "4. Random forest classifier can handle the missing values. There are two ways to handle the missing values. First is to use median values to replace continuous variables and second is to compute the proximity-weighted average of missing values.\n",
    "5. Random forest classifier can be used for feature selection. It means selecting the most important features out of the available features from the training dataset.\n",
    "\n",
    "\n",
    "The disadvantages of Random Forest algorithm are listed below:-\n",
    "\n",
    "\n",
    "1. The biggest disadvantage of random forests is its computational complexity. Random forests is very slow in making predictions because large number of decision-trees are used to make predictions. All the trees in the forest have to make a prediction for the same input and then perform voting on it. So, it is a time-consuming process.\n",
    "2. The model is difficult to interpret as compared to a decision-tree, where we can easily make a prediction as compared to a decision-tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48721762-e2fb-48ac-bdce-e56cc0ada577",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Feature selection with Random Forests\n",
    "\n",
    "Random forests algorithm can be used for feature selection process. This algorithm can be used to rank the importance of variables in a regression or classification problem. \n",
    "\n",
    "We measure the variable importance in a dataset by fitting the random forest algorithm to the data. During the fitting process, the out-of-bag error for each data point is recorded and averaged over the forest. \n",
    "\n",
    "The importance of the j-th feature was measured after training. The values of the j-th feature were permuted among the training data and the out-of-bag error was again computed on this perturbed dataset. The importance score for the j-th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees. The score is normalized by the standard deviation of these differences.\n",
    "\n",
    "Features which produce large values for this score are ranked as more important than features which produce small values. Based on this score, we will choose the most important features and drop the least important ones for model building. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e243083e-91aa-4e64-bc6e-4a6e0247a304",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5. Difference between Random Forests and Decision Trees\n",
    "\n",
    "I will compare random forests with decision-trees. Some salient features of comparison are as follows:-\n",
    "\n",
    "1. Random forests is a set of multiple decision-trees.\n",
    "\n",
    "2. Decision-trees are computationally faster as compared to random forests.\n",
    "\n",
    "3. Deep decision-trees may suffer from overfitting. Random forest prevents overfitting by creating trees on random forests.\n",
    "\n",
    "4. Random forest is difficult to interpret. But, a decision-tree is easily interpretable and can be converted to rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5864d70-0cad-45c4-ba52-a54d468eb990",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number, lit\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0beeb9-a2de-49e1-8e5d-8a1942d08c1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName('RandomForest').getOrCreate()\n",
    "\n",
    "# File path in DBFS\n",
    "file_path = \"dbfs:/FileStore/MLFlowRandomForestClassifierTutorial/car_evaluation.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Define the new column names\n",
    "col_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
    "\n",
    "# Rename columns using toDF\n",
    "df = df.toDF(*col_names)\n",
    "\n",
    "# StringIndexer Initialization\n",
    "categorical_columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_indexed\") for column in categorical_columns]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df = pipeline.fit(df).transform(df)\n",
    "\n",
    "df = df.select('buying_indexed', 'maint_indexed', 'doors_indexed', 'persons_indexed', 'lug_boot_indexed', 'safety_indexed', 'class_indexed')\n",
    "\n",
    "# Rename columns using toDF\n",
    "df = df.toDF(*col_names)\n",
    "df = df.withColumnRenamed(\"class\", \"label\")\n",
    "\n",
    "# Show the DataFrame with new column names\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b7dee04-3bea-4b58-8b90-25f88faee704",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 6. Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0280a58b-40a1-49c8-a837-df3f9281686d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6.1 Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "912f0302-9a89-4b0e-a756-88db5aede6d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------+--------+------+-----+\n|buying|maint|doors|persons|lug_boot|safety|label|\n+------+-----+-----+-------+--------+------+-----+\n|   0.0|  0.0|  0.0|    0.0|     0.0|   0.0|  1.0|\n|   0.0|  0.0|  0.0|    0.0|     0.0|   1.0|  1.0|\n|   0.0|  0.0|  0.0|    0.0|     1.0|   0.0|  1.0|\n|   0.0|  0.0|  0.0|    0.0|     1.0|   1.0|  0.0|\n|   0.0|  0.0|  0.0|    0.0|     1.0|   2.0|  0.0|\n|   0.0|  0.0|  0.0|    0.0|     2.0|   1.0|  0.0|\n|   0.0|  0.0|  0.0|    1.0|     0.0|   1.0|  1.0|\n|   0.0|  0.0|  0.0|    1.0|     0.0|   2.0|  0.0|\n|   0.0|  0.0|  0.0|    1.0|     1.0|   0.0|  1.0|\n|   0.0|  0.0|  0.0|    1.0|     2.0|   1.0|  0.0|\n|   0.0|  0.0|  0.0|    1.0|     2.0|   2.0|  0.0|\n|   0.0|  0.0|  0.0|    2.0|     0.0|   0.0|  0.0|\n|   0.0|  0.0|  0.0|    2.0|     0.0|   2.0|  0.0|\n|   0.0|  0.0|  0.0|    2.0|     1.0|   1.0|  0.0|\n|   0.0|  0.0|  0.0|    2.0|     2.0|   1.0|  0.0|\n|   0.0|  0.0|  0.0|    2.0|     2.0|   2.0|  0.0|\n|   0.0|  0.0|  1.0|    0.0|     0.0|   0.0|  1.0|\n|   0.0|  0.0|  1.0|    0.0|     1.0|   1.0|  1.0|\n|   0.0|  0.0|  1.0|    0.0|     2.0|   0.0|  1.0|\n|   0.0|  0.0|  1.0|    1.0|     0.0|   0.0|  1.0|\n+------+-----+-----+-------+--------+------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Define the feature and label columns & Assemble the feature vector\n",
    "feature_columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\",featuresCol=\"features\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)\n",
    "train_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da6c9d62-c644-49d1-98be-1aae51c7714f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6.2 Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67952515-aa97-4ed1-b8da-c1688be67056",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler,rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31ab6750-2d81-4f12-b827-751ea4d2b126",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6.3 Hyperparameter Tuning and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d58df00b-ae65-4ec0-a609-d213a04b004e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "# Create the cross-validator\n",
    "cross_validator = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\"),\n",
    "                          numFolds=5, seed=42)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "cv_model = cross_validator.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e37c4942-1db2-4e47-9dae-835da300acb4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6.4 Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40172c10-02db-4fbb-802f-e509f3321834",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.97\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = cv_model.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = {:.2f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main.py",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
